{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Домашняя работа по теме AOT классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание: выполнить классификацию fakenews 3мя разными способами. Получить на задаче классификации значение f1 выше 0.91 для методов sklearn  и выше 0.52 для методов на pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "stops_en = stopwords.words(\"english\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% [..........................................................................] 1253562 / 1253562"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Constraint_Train (3).csv'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wget\n",
    "#wget.download('https://raw.githubusercontent.com/diptamath/covid_fake_news/main/data/Constraint_Train.csv', 'Constraint_Train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Constraint_Train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The CDC currently reports 99031 deaths. In gen...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>States reported 1121 deaths a small rise from ...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Politically Correct Woman (Almost) Uses Pandem...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              tweet label\n",
       "0   1  The CDC currently reports 99031 deaths. In gen...  real\n",
       "1   2  States reported 1121 deaths a small rise from ...  real\n",
       "2   3  Politically Correct Woman (Almost) Uses Pandem...  fake"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Politically Correct Woman (Almost) Uses Pandemic as Excuse Not to Reuse Plastic Bag https://t.co/thF8GuNFPe #coronavirus #nashville'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tweet[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "real    3360\n",
       "fake    3060\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Классы сбалансированы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6420 entries, 0 to 6419\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      6420 non-null   int64 \n",
      " 1   tweet   6420 non-null   object\n",
      " 2   label   6420 non-null   object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 150.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "типы данных в порядке, нулевые значения отсутствуют"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Выполним токенизацию текста для последующей работы:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Не будем удалять знаки пунктуации, символы, стоп слова и выполнять лемматизацию, так как все эти нюансы нейровнная сеть может испольовать как основову для выявления паттернов. Приведем только слова в нижний регистр"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 6420/6420 [00:02<00:00, 2795.74it/s]\n"
     ]
    }
   ],
   "source": [
    "tweets_asis = [word_tokenize(text.lower()) for text in tqdm(df.tweet)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the',\n",
       "  'cdc',\n",
       "  'currently',\n",
       "  'reports',\n",
       "  '99031',\n",
       "  'deaths',\n",
       "  '.',\n",
       "  'in',\n",
       "  'general',\n",
       "  'the',\n",
       "  'discrepancies',\n",
       "  'in',\n",
       "  'death',\n",
       "  'counts',\n",
       "  'between',\n",
       "  'different',\n",
       "  'sources',\n",
       "  'are',\n",
       "  'small',\n",
       "  'and',\n",
       "  'explicable',\n",
       "  '.',\n",
       "  'the',\n",
       "  'death',\n",
       "  'toll',\n",
       "  'stands',\n",
       "  'at',\n",
       "  'roughly',\n",
       "  '100000',\n",
       "  'people',\n",
       "  'today',\n",
       "  '.'],\n",
       " ['states',\n",
       "  'reported',\n",
       "  '1121',\n",
       "  'deaths',\n",
       "  'a',\n",
       "  'small',\n",
       "  'rise',\n",
       "  'from',\n",
       "  'last',\n",
       "  'tuesday',\n",
       "  '.',\n",
       "  'southern',\n",
       "  'states',\n",
       "  'reported',\n",
       "  '640',\n",
       "  'of',\n",
       "  'those',\n",
       "  'deaths',\n",
       "  '.',\n",
       "  'https',\n",
       "  ':',\n",
       "  '//t.co/yasgrtt4ux']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_asis[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Лемматизируем твиты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_to_wordlist(review, lemmatizer = None, remove_stopwords=False):\n",
    "    # разбиваем обзор на слова\n",
    "    words = word_tokenize(review.lower().strip())\n",
    "    for word in words:\n",
    "        if remove_stopwords:\n",
    "            words = [w for w in words if not w in stops_en]\n",
    "    lemmatized_word=[]\n",
    "    if lemmatizer:\n",
    "        for word in words:\n",
    "            lemmatized_word.append(lemmatizer.lemmatize(word))\n",
    "        return lemmatized_word\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "#чтобы различать знаки пунктуации\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 6420/6420 [00:23<00:00, 270.46it/s]\n"
     ]
    }
   ],
   "source": [
    "tweets_lem_nosw = [review_to_wordlist(review, lemmatizer, remove_stopwords = True) for review in tqdm(df.tweet)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. удалим стоп-слова и знаки пунктуации + лемматизация:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 6420/6420 [00:03<00:00, 1836.04it/s]\n"
     ]
    }
   ],
   "source": [
    "tweets_lem = [review_to_wordlist(review, lemmatizer, remove_stopwords = False) for review in tqdm(df.tweet)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем применить мешок слов для для выделения фич юни-би-грамм:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizerCV1 = CountVectorizer(ngram_range=(1, 1))\n",
    "vectorizerCV2 = CountVectorizer(ngram_range=(2, 2))\n",
    "Xcv1_asis = vectorizerCV1.fit_transform([' '.join(tweet) for tweet in tweets_asis])\n",
    "Xcv2_asis = vectorizerCV2.fit_transform([' '.join(tweet) for tweet in tweets_asis])\n",
    "Xcv1_lem = vectorizerCV1.fit_transform([' '.join(tweet) for tweet in tweets_lem])\n",
    "Xcv2_lem = vectorizerCV2.fit_transform([' '.join(tweet) for tweet in tweets_lem])\n",
    "Xcv1_lem_nostops = vectorizerCV1.fit_transform([' '.join(tweet) for tweet in tweets_lem_nosw])\n",
    "Xcv2_lem_nostops = vectorizerCV2.fit_transform([' '.join(tweet) for tweet in tweets_lem_nosw])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = (df.label == 'real').astype(int).to_list()\n",
    "X_train_cv1_asis, X_test_cv1_asis, y_train_cv1_asis, y_test_cv1_asis = train_test_split(Xcv1_asis, labels, test_size=0.2, random_state=42, stratify=labels)\n",
    "X_train_cv2_asis, X_test_cv2_asis, y_train_cv2_asis, y_test_cv2_asis = train_test_split(Xcv2_asis, labels, test_size=0.2, random_state=42, stratify=labels)\n",
    "X_train_cv1_lem, X_test_cv1_lem, y_train_cv1_lem, y_test_cv1_lem = train_test_split(Xcv1_lem, labels, test_size=0.2, random_state=42, stratify=labels)\n",
    "X_train_cv2_lem, X_test_cv2_lem, y_train_cv2_lem, y_test_cv2_lem = train_test_split(Xcv2_lem, labels, test_size=0.2, random_state=42, stratify=labels)\n",
    "X_train_cv1_lem_nostops, X_test_cv1_lem_nostops, y_train_cv1_lem_nostops, y_test_cv1_lem_nostops = train_test_split(Xcv1_lem_nostops, labels, test_size=0.2, random_state=42, stratify=labels)\n",
    "X_train_cv2_lem_nostops, X_test_cv2_lem_nostops, y_train_cv2_lem_nostops, y_test_cv2_lem_nostops = train_test_split(Xcv2_lem_nostops, labels, test_size=0.2, random_state=42, stratify=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Воспользуемся в качестве бэйзлайна догистической регрессией для классификации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.94      0.92       612\n",
      "           1       0.94      0.92      0.93       672\n",
      "\n",
      "    accuracy                           0.93      1284\n",
      "   macro avg       0.93      0.93      0.93      1284\n",
      "weighted avg       0.93      0.93      0.93      1284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#просто токенизация с юни-граммами\n",
    "model_asis = LogisticRegression(random_state = 42)\n",
    "model_asis.fit(X_train_cv1_asis, y_train_cv1_asis)\n",
    "predicted_asis = model_asis.predict(X_test_cv1_asis)\n",
    "print(classification_report(y_test_cv1_asis, predicted_asis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.92      0.88       612\n",
      "           1       0.92      0.85      0.88       672\n",
      "\n",
      "    accuracy                           0.88      1284\n",
      "   macro avg       0.89      0.89      0.88      1284\n",
      "weighted avg       0.89      0.88      0.88      1284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#просто токенизации с биграммами\n",
    "model_asis2 = LogisticRegression(random_state = 42)\n",
    "model_asis2.fit(X_train_cv2_asis, y_train_cv2_asis)\n",
    "predicted_asis2 = model_asis2.predict(X_test_cv2_asis)\n",
    "print(classification_report(y_test_cv2_asis, predicted_asis2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "похоже биграммы показывают более худшй результат, поэтому далее будем применять юни-граммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.94      0.93       612\n",
      "           1       0.95      0.92      0.93       672\n",
      "\n",
      "    accuracy                           0.93      1284\n",
      "   macro avg       0.93      0.93      0.93      1284\n",
      "weighted avg       0.93      0.93      0.93      1284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#лемматизированные твиты\n",
    "model_lem = LogisticRegression(random_state = 42)\n",
    "model_lem.fit(X_train_cv1_lem, y_train_cv1_lem)\n",
    "predicted_lem = model_lem.predict(X_test_cv1_lem)\n",
    "print(classification_report(y_test_cv1_lem, predicted_lem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.95      0.93       612\n",
      "           1       0.95      0.92      0.94       672\n",
      "\n",
      "    accuracy                           0.93      1284\n",
      "   macro avg       0.93      0.94      0.93      1284\n",
      "weighted avg       0.94      0.93      0.93      1284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#лемматизированные и без стопслов твиты\n",
    "model_lem_nostops = LogisticRegression(random_state = 42)\n",
    "model_lem_nostops.fit(X_train_cv1_lem_nostops, y_train_cv1_lem_nostops)\n",
    "predicted_lem_nostops = model_lem_nostops.predict(X_test_cv1_lem_nostops)\n",
    "print(classification_report(y_test_cv1_lem_nostops, predicted_lem_nostops))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим покажут ли другие методы классификации лучший результат:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_lr = LogisticRegression()\n",
    "clf_knn = KNeighborsClassifier()\n",
    "clf_rf = RandomForestClassifier()\n",
    "clf_ri = RidgeClassifier()\n",
    "clf_svc = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "params1 = {'penalty': ['l1','l2'], 'C': [0.001,0.01,0.1,1,10,30]}\n",
    "params2 = {'n_neighbors': range(1,10)}\n",
    "params3 = {'max_depth': range(2,20,3),\n",
    "            'n_estimators': [5,10,15,20]}\n",
    "params4 = {'alpha': [0.1, 0.2, 0.5, 1.0, 5., 10., 30.]}\n",
    "params5 = {'C': [0.001,0.01,0.1,1,10, 50], \n",
    "          'gamma': [0.001, 0.01, 0.1, 1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression : {'C': 10, 'penalty': 'l2'} \t acc: 0.9330242489058792\n",
      "KNeighborsClassifier : {'n_neighbors': 1} \t acc: 0.7899230891755978\n",
      "RandomForestClassifier : {'max_depth': 17, 'n_estimators': 20} \t acc: 0.8742208417715279\n",
      "RidgeClassifier : {'alpha': 10.0} \t acc: 0.9308837918401711\n",
      "SVC : {'C': 50, 'gamma': 0.01} \t acc: 0.9378914753377174\n"
     ]
    }
   ],
   "source": [
    "best_estimators = []\n",
    "for clf, params_dict in zip([clf_lr, clf_knn, clf_rf, clf_ri, clf_svc], [params1, params2, params3, params4, params5]):\n",
    "    grid = GridSearchCV(clf, params_dict, cv=10, scoring='accuracy')\n",
    "    grid.fit(X_train_cv1_asis, y_train_cv1_asis)\n",
    "    print(clf.__class__.__name__, ':', grid.best_params_,\"\\t\", 'acc:', grid.best_score_)\n",
    "\n",
    "    best_estimators.append(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вывод: метов опорных вектором показал наилучший результат, однако логистическая регрессия показала очень близкий результат. Среди всех вариантов обработанных твитов наилучший показывает лемматизированный и с удалением стоп слов. результат f1 лишь чуть-чуть лучше 93(0)-94(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Подготовим фичи применив векторизацию с помощью tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizerTfidf = TfidfVectorizer(ngram_range=(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtfidf_asis = vectorizerTfidf.fit_transform([' '.join(tweet) for tweet in tweets_asis])\n",
    "Xtfidf_lem = vectorizerTfidf.fit_transform([' '.join(tweet) for tweet in tweets_lem])\n",
    "Xtfidf_lem_nostops = vectorizerTfidf.fit_transform([' '.join(tweet) for tweet in tweets_lem_nosw])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf_asis, X_test_tfidf_asis, y_train_tfidf_asis, y_test_tfidf_asis = train_test_split(Xtfidf_asis, labels, test_size=0.2, random_state=42, stratify=labels)\n",
    "X_train_tfidf_lem, X_test_tfidf_lem, y_train_tfidf_lem, y_test_tfidf_lem = train_test_split(Xtfidf_lem, labels, test_size=0.2, random_state=42, stratify=labels)\n",
    "X_train_tfidf_lem_nostops, X_test_tfidf_lem_nostops, y_train_tfidf_lem_nostops, y_test_tfidf_lem_nostops = train_test_split(Xtfidf_lem_nostops, labels, test_size=0.2, random_state=42, stratify=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.94      0.92       612\n",
      "           1       0.94      0.91      0.93       672\n",
      "\n",
      "    accuracy                           0.92      1284\n",
      "   macro avg       0.92      0.92      0.92      1284\n",
      "weighted avg       0.92      0.92      0.92      1284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#просто токенизация с юни-граммами\n",
    "model_tfidf_asis = LogisticRegression(random_state = 42)\n",
    "model_tfidf_asis.fit(X_train_tfidf_asis, y_train_tfidf_asis)\n",
    "predicted_tfidf_asis = model_tfidf_asis.predict(X_test_tfidf_asis)\n",
    "print(classification_report(y_test_tfidf_asis, predicted_tfidf_asis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.94      0.92       612\n",
      "           1       0.94      0.91      0.93       672\n",
      "\n",
      "    accuracy                           0.92      1284\n",
      "   macro avg       0.92      0.92      0.92      1284\n",
      "weighted avg       0.92      0.92      0.92      1284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#лемматизированные твиты\n",
    "model_tfidf_lem = LogisticRegression(random_state = 42)\n",
    "model_tfidf_lem.fit(X_train_tfidf_lem, y_train_tfidf_lem)\n",
    "predicted_tfidf_lem = model_tfidf_lem.predict(X_test_tfidf_lem)\n",
    "print(classification_report(y_test_tfidf_lem, predicted_tfidf_lem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.94      0.93       612\n",
      "           1       0.95      0.92      0.93       672\n",
      "\n",
      "    accuracy                           0.93      1284\n",
      "   macro avg       0.93      0.93      0.93      1284\n",
      "weighted avg       0.93      0.93      0.93      1284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##лемматизированные и без стоп-слов твиты\n",
    "model_tfidf_lem_nostops = LogisticRegression(random_state = 42)\n",
    "model_tfidf_lem_nostops.fit(X_train_tfidf_lem_nostops, y_train_tfidf_lem_nostops)\n",
    "predicted_tfidf_lem_nostops = model_tfidf_lem_nostops.predict(X_test_tfidf_lem_nostops)\n",
    "print(classification_report(y_test_tfidf_lem_nostops, predicted_tfidf_lem_nostops))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вывод: метод вектризации слов tf-idf выдал такой же результат как и обычный мешок слов и даже немного ниже"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Подготовим фичи применив векторизацию с помощью word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_to_wordlist1(review, lemmatizer, remove_stopwords=False):\n",
    "    # убираем ссылки\n",
    "    #review_text = re.sub(r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\", \" \", review)\n",
    "    # достаем сам текст\n",
    "    # review_text = BeautifulSoup(review_text, \"lxml\").get_text()\n",
    "    # оставляем только буквенные символы\n",
    "    #review_text = re.sub(\"[^-яА-ЯёЁ]\",\" \", review_text)\n",
    "    # приводим к нижнему регистру и разбиваем на слова по символу пробела\n",
    "    words = word_tokenize(review.lower())\n",
    "    if remove_stopwords: # убираем стоп-слова\n",
    "        words = [w for w in words if not w in stops_en]\n",
    "    if lemmatizer:\n",
    "        return([lemmatizer.lemmatize(word) for word in words])\n",
    "    else:\n",
    "        return words\n",
    "    \n",
    "def review_to_sentences(review, tokenizer, lemmatizer = None, remove_stopwords=False):\n",
    "    # разбиваем обзор на предложения\n",
    "    raw_sentences = sent_tokenize(review.strip())\n",
    "    sentences = []\n",
    "    # применяем предыдущую функцию к каждому предложению\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append(review_to_wordlist1(raw_sentence, lemmatizer, remove_stopwords))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 6420/6420 [00:02<00:00, 2241.81it/s]\n"
     ]
    }
   ],
   "source": [
    "#просто токенизированные слова\n",
    "sent_asis = []\n",
    "for review in tqdm(df.tweet):\n",
    "    sent_asis += review_to_sentences(review, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Wall time: 952 ms\n"
     ]
    }
   ],
   "source": [
    "print(\"Training model...\")\n",
    "\n",
    "%time model_w2v_asis = Word2Vec(sent_asis, workers=4, vector_size=300, min_count=10, window=10, sample=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 6420/6420 [00:04<00:00, 1597.51it/s]\n"
     ]
    }
   ],
   "source": [
    "sent_lem = []\n",
    "for review in tqdm(df.tweet):\n",
    "    sent_lem += review_to_sentences(review, tokenizer, lemmatizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Wall time: 973 ms\n"
     ]
    }
   ],
   "source": [
    "print(\"Training model...\")\n",
    "\n",
    "%time model_w2v_lem = Word2Vec(sent_lem, workers=4, vector_size=300, min_count=10, window=10, sample=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 6420/6420 [00:04<00:00, 1475.15it/s]\n"
     ]
    }
   ],
   "source": [
    "sent_lem_nostops = []\n",
    "for review in tqdm(df.tweet):\n",
    "    sent_lem_nostops += review_to_sentences(review, tokenizer, lemmatizer, remove_stopwords = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Wall time: 729 ms\n"
     ]
    }
   ],
   "source": [
    "print(\"Training model...\")\n",
    "\n",
    "%time model_w2v_lem_nostops = Word2Vec(sent_lem_nostops, workers=4, vector_size=300, min_count=10, window=10, sample=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xw2v_asis = vectorizerCV1.fit_transform([' '.join(tweet) for tweet in tweets_asis])\n",
    "Xw2v_lem = vectorizerCV1.fit_transform([' '.join(tweet) for tweet in tweets_lem])\n",
    "Xw2v_lem_nostops = vectorizerCV1.fit_transform([' '.join(tweet) for tweet in tweets_lem_nosw])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_w2v_asis = []\n",
    "vocab = model_w2v_asis.wv.index_to_key\n",
    "for tweet in tweets_asis:\n",
    "    tweet_embeddings = []\n",
    "    for word in tweet:\n",
    "        if word in vocab:\n",
    "            tweet_embeddings.append(model_w2v_asis.wv[word])\n",
    "    X_w2v_asis.append(tweet_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_w2v_lem = []\n",
    "vocab = model_w2v_lem.wv.index_to_key\n",
    "for tweet in tweets_lem:\n",
    "    tweet_embeddings = []\n",
    "    for word in tweet:\n",
    "        if word in vocab:\n",
    "            tweet_embeddings.append(model_w2v_lem.wv[word])\n",
    "    X_w2v_lem.append(tweet_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_w2v_lem_nostops = []\n",
    "vocab = model_w2v_lem_nostops.wv.index_to_key\n",
    "for tweet in tweets_lem_nosw:\n",
    "    tweet_embeddings = []\n",
    "    for word in tweet:\n",
    "        if word in vocab:\n",
    "            tweet_embeddings.append(model_w2v_lem_nostops.wv[word])\n",
    "    X_w2v_lem_nostops.append(tweet_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#для логистической регрессии суммируем все эмбединги одного твита.\n",
    "X_w2v_lem_nostops1 = []\n",
    "vocab = model_w2v_lem_nostops.wv.index_to_key\n",
    "for tweet in tweets_lem_nosw:\n",
    "    tweet_embeddings = []\n",
    "    for word in tweet:\n",
    "        if word in vocab:\n",
    "            tweet_embeddings.append(model_w2v_lem_nostops.wv[word])\n",
    "    if len(tweet_embeddings):\n",
    "        tweet_embeddings = np.sum(tweet_embeddings, axis=0)\n",
    "    else:\n",
    "        tweet_embeddings = np.zeros(300)\n",
    "    X_w2v_lem_nostops1.append(tweet_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.91      0.90       612\n",
      "           1       0.91      0.90      0.91       672\n",
      "\n",
      "    accuracy                           0.90      1284\n",
      "   macro avg       0.90      0.90      0.90      1284\n",
      "weighted avg       0.90      0.90      0.90      1284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train_w2v_lem_nostops, X_test_w2v_lem_nostops, y_train_w2v_lem_nostops, y_test_w2v_lem_nostops = train_test_split(X_w2v_lem_nostops1, labels, test_size=0.2, random_state=42, stratify=labels)\n",
    "lr_w2v_lem_nostops = LogisticRegression(random_state = 42)\n",
    "lr_w2v_lem_nostops.fit(X_train_w2v_lem_nostops, y_train_w2v_lem_nostops)\n",
    "predicted_w2v_lem_nostops = lr_w2v_lem_nostops.predict(X_test_w2v_lem_nostops)\n",
    "print(classification_report(y_test_w2v_lem_nostops, predicted_w2v_lem_nostops))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вывод: результат применения сумы эмбдингов для твита в логистической регрессии оказался ожидаемо ниже, чем чем при использовании обычного мешка солв"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Применим нейронные сети для предсказаия тональности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "посмотрим каково распределние длинн предолжений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "fd = Counter([len(embed) for embed in X_w2v_lem_nostops])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(16,4))\n",
    "plt.plot(list(fd.keys()), list(fd.values()), 'ro')\n",
    "#plt.xlim([0,100])\n",
    "#plt.ylim([0,300])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(16,4))\n",
    "plt.plot(list(fd.keys()), list(fd.values()), 'ro')\n",
    "plt.xlim([0,70])\n",
    "#plt.ylim([0,300])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из графиков видно, что частота твитов начиная от длинн более 4х, но менее 45 слов превышает 15 (что уже достаточно для батчей). Таким образом пэддинг не будет сильно влиять. В данной работе принимимае, что у нас будет один батч и максимальная динна предолжения 50 слов. Предложения, длиянной менее 50 слов  будем дозаполнятьнулевыми эмбединг-векторами (пэддинг) до 50ти. Для того, чтобы делать пэддинг используем функции pad_sequence (заполнение пэддинг) и pack_padded_sequence (для упаковки и одновременной подачи - ниже пример работы механизма). Судя из статей эти функции указывают lstm на пэддинги и она не учитывает их при тренировке.\n",
    "Напишем класс для модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class Net(nn.Module):\n",
    "    #lstm_units - количество lstm ячеек, то есть длина вектора на выходе lstm слоя\n",
    "    #embeddings_dim - длинна эмбединга\n",
    "    #описываем фнукцию класса Net, которая будет вызываться автоматичесики при создании экземпляра объекта класса\n",
    "    def __init__(self, \n",
    "                 embedding_dim, \n",
    "                 lstm_units, \n",
    "                 batch_size, \n",
    "                 lstm_layers = 1, \n",
    "                 bidirectional = False, \n",
    "                 hidden_dim = 128,\n",
    "                 dropout = 0):\n",
    "        super(Net, self).__init__()\n",
    "        #создаем переменную и инициализиуем слой lstm\n",
    "        #batch_fist = True говорит о том, что мы будем использовать упаковку и заполнение нелувыми эмбеингами педложения с меньшим количество слов, чем мксимальное в батче\n",
    "        self.lstm = nn.LSTM(embedding_dim, lstm_units, num_layers = lstm_layers, bidirectional = bidirectional ,batch_first=True)\n",
    "        #создаем переменную и инициализируем линейный слой. 1 - значит длина вектора на выходе (float от 0 до 1)\n",
    "        self.num_directions = 2 if bidirectional == True else 1\n",
    "        self.sigm = nn.Sigmoid()\n",
    "        #self.relu = nn.ReLU()\n",
    "        self.fc = nn.Linear(lstm_units * self.num_directions, 1)\n",
    "        #self.fc1 = nn.Linear(lstm_units * self.num_directions, hidden_dim)\n",
    "        #self.fc2 = nn.Linear(hidden_dim, 1)\n",
    "        #self.dropout = nn.Dropout(dropout)\n",
    "        #self.hidden_dim = hidden_dim\n",
    "        self.lstm_layers = lstm_layers\n",
    "        self.lstm_units = lstm_units\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "    #фукнция инициализации h0, c0\n",
    "    def init_hidden(self, batch_size):\n",
    "        h, c = (Variable(torch.zeros(self.lstm_layers * self.num_directions, batch_size, self.lstm_units)),\n",
    "                Variable(torch.zeros(self.lstm_layers * self.num_directions, batch_size, self.lstm_units)))\n",
    "        return h, c\n",
    "        \n",
    "    def forward(self, tweets, text_length):\n",
    "        batch_size = len(tweets)\n",
    "        h_0, c_0 = self.init_hidden(batch_size)\n",
    "        #добавляем нулевые эмбединги для твитов с меньшей длинной чем максимальное в батче\n",
    "        padded = pad_sequence(tweets, batch_first=True)\n",
    "        tweet_lengths = [len(i) for i in tweets] #список длин предложений\n",
    "        #сортируем по убыванию и упаковываем твиты (что-то в жухе танспоирования). \n",
    "        #на выходе 2 тензора и второй описывает сколько твитов имеет ненулевые эмбединги на каждой определенной позиции\n",
    "        packed_embedded = pack_padded_sequence(padded, tweet_lengths, batch_first=True, enforce_sorted=False) #по умолчанию сортируетпо убыванию длинны\n",
    "        output, (shortterm, longterm) = self.lstm(packed_embedded, (h_0, c_0))\n",
    "        #распаковываем обратно (pad нули будут присутствовать)\n",
    "        output_unpacked, output_lengths = pad_packed_sequence(output, batch_first=True)\n",
    "        out = output_unpacked[:, -1, :]\n",
    "        #если дальше испольовем output, то back propogation не работает, то есть всегда один и тот же результат. а вот с состояними longterm все вычисляется\n",
    "        if self.bidirectional:\n",
    "            longterm = torch.cat((longterm[-2, :, :], longterm[-1, :, :]), dim=1)\n",
    "        prediction = self.sigm(self.fc(longterm))\n",
    "        #почему-то получаются отрицтальные значения и я не знаю что с ними делать\n",
    "        #sig = self.relu(longterm)\n",
    "        #dense1 = self.fc1(sig)\n",
    "        #drop = self.dropout(dense1)\n",
    "        #prediction = self.fc2(drop)\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "так как мы будем испоьлзовать максимальную длинну предложения, то отфильтруем (удалим) сильно длиынне предолжения, которые встречаются редко. определеяем по графику, что хвост с частотой встречаемости длинн больше 45 стремится к нулю. берем максимльную длинну 50 слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 50\n",
    "index_norm = [i for i, tweet in enumerate(X_w2v_lem_nostops) if (len(tweet) <= max_len) and (len(tweet) != 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "количество удаленных твитов-выбросов:  11\n"
     ]
    }
   ],
   "source": [
    "print( \"количество удаленных твитов-выбросов: \", len ([i for i, tweet in enumerate(X_w2v_lem_nostops) if len(tweet) > max_len]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_norm = list(np.array(X_w2v_lem_nostops)[index_norm])\n",
    "y_norm = list(np.array(labels)[index_norm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_norm, y_norm, test_size=0.2, random_state=42, stratify=y_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_t = [torch.tensor(emb_tweet) for emb_tweet in X_train]\n",
    "X_test_t = [torch.tensor(emb_tweet) for emb_tweet in X_test]\n",
    "y_train_t = torch.tensor(y_train).float()\n",
    "y_test_t = torch.tensor(y_test).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5126"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#фукнция расчета точности предсказания accuracy\n",
    "def accuracy(probs, target):\n",
    "    correct = (probs > 0.5) == target\n",
    "    accuracy = correct.sum().item() / len(target)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем код для тренировки и оценки модели bidirectional lstm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7012, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch  0\n",
      "\tTrain Loss: 0.701 | Train Acc: 49.86%\n",
      "\t Val. Loss: 0.672 |  Val. Acc: 71.45%\n",
      "tensor(0.6713, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.671 | Train Acc: 71.75%\n",
      "\t Val. Loss: 0.651 |  Val. Acc: 72.70%\n",
      "tensor(0.6497, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.650 | Train Acc: 75.97%\n",
      "\t Val. Loss: 0.635 |  Val. Acc: 73.09%\n",
      "tensor(0.6328, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.633 | Train Acc: 76.10%\n",
      "\t Val. Loss: 0.621 |  Val. Acc: 73.24%\n",
      "tensor(0.6187, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.619 | Train Acc: 76.28%\n",
      "\t Val. Loss: 0.610 |  Val. Acc: 73.79%\n",
      "tensor(0.6063, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.606 | Train Acc: 76.55%\n",
      "\t Val. Loss: 0.599 |  Val. Acc: 74.02%\n",
      "tensor(0.5950, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.595 | Train Acc: 76.65%\n",
      "\t Val. Loss: 0.589 |  Val. Acc: 74.18%\n",
      "tensor(0.5847, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.585 | Train Acc: 76.82%\n",
      "\t Val. Loss: 0.580 |  Val. Acc: 74.57%\n",
      "tensor(0.5752, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.575 | Train Acc: 76.96%\n",
      "\t Val. Loss: 0.571 |  Val. Acc: 74.88%\n",
      "tensor(0.5662, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.566 | Train Acc: 77.25%\n",
      "\t Val. Loss: 0.563 |  Val. Acc: 74.88%\n",
      "tensor(0.5578, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch  10\n",
      "\tTrain Loss: 0.558 | Train Acc: 77.45%\n",
      "\t Val. Loss: 0.555 |  Val. Acc: 75.04%\n",
      "tensor(0.5498, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.550 | Train Acc: 77.55%\n",
      "\t Val. Loss: 0.548 |  Val. Acc: 75.12%\n",
      "tensor(0.5423, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.542 | Train Acc: 77.76%\n",
      "\t Val. Loss: 0.541 |  Val. Acc: 75.04%\n",
      "tensor(0.5352, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.535 | Train Acc: 77.86%\n",
      "\t Val. Loss: 0.534 |  Val. Acc: 75.04%\n",
      "tensor(0.5284, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.528 | Train Acc: 77.94%\n",
      "\t Val. Loss: 0.528 |  Val. Acc: 75.51%\n",
      "tensor(0.5220, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.522 | Train Acc: 78.05%\n",
      "\t Val. Loss: 0.522 |  Val. Acc: 75.59%\n",
      "tensor(0.5159, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.516 | Train Acc: 78.19%\n",
      "\t Val. Loss: 0.516 |  Val. Acc: 75.82%\n",
      "tensor(0.5100, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.510 | Train Acc: 78.29%\n",
      "\t Val. Loss: 0.511 |  Val. Acc: 76.21%\n",
      "tensor(0.5045, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.504 | Train Acc: 78.50%\n",
      "\t Val. Loss: 0.505 |  Val. Acc: 76.44%\n",
      "tensor(0.4992, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.499 | Train Acc: 78.68%\n",
      "\t Val. Loss: 0.500 |  Val. Acc: 76.76%\n",
      "tensor(0.4941, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch  20\n",
      "\tTrain Loss: 0.494 | Train Acc: 78.95%\n",
      "\t Val. Loss: 0.495 |  Val. Acc: 76.68%\n",
      "tensor(0.4892, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.489 | Train Acc: 79.03%\n",
      "\t Val. Loss: 0.490 |  Val. Acc: 76.99%\n",
      "tensor(0.4846, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.485 | Train Acc: 79.18%\n",
      "\t Val. Loss: 0.486 |  Val. Acc: 77.07%\n",
      "tensor(0.4801, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.480 | Train Acc: 79.26%\n",
      "\t Val. Loss: 0.481 |  Val. Acc: 76.99%\n",
      "tensor(0.4758, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.476 | Train Acc: 79.26%\n",
      "\t Val. Loss: 0.477 |  Val. Acc: 77.30%\n",
      "tensor(0.4717, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.472 | Train Acc: 79.40%\n",
      "\t Val. Loss: 0.473 |  Val. Acc: 77.30%\n",
      "tensor(0.4677, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.468 | Train Acc: 79.79%\n",
      "\t Val. Loss: 0.469 |  Val. Acc: 77.46%\n",
      "tensor(0.4639, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.464 | Train Acc: 79.96%\n",
      "\t Val. Loss: 0.465 |  Val. Acc: 77.54%\n",
      "tensor(0.4603, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.460 | Train Acc: 80.12%\n",
      "\t Val. Loss: 0.461 |  Val. Acc: 77.85%\n",
      "tensor(0.4567, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.457 | Train Acc: 80.30%\n",
      "\t Val. Loss: 0.458 |  Val. Acc: 77.93%\n",
      "tensor(0.4533, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch  30\n",
      "\tTrain Loss: 0.453 | Train Acc: 80.55%\n",
      "\t Val. Loss: 0.454 |  Val. Acc: 78.24%\n",
      "tensor(0.4501, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.450 | Train Acc: 80.53%\n",
      "\t Val. Loss: 0.451 |  Val. Acc: 78.71%\n",
      "tensor(0.4469, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.447 | Train Acc: 80.65%\n",
      "\t Val. Loss: 0.448 |  Val. Acc: 78.94%\n",
      "tensor(0.4438, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.444 | Train Acc: 80.57%\n",
      "\t Val. Loss: 0.444 |  Val. Acc: 79.17%\n",
      "tensor(0.4409, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.441 | Train Acc: 80.69%\n",
      "\t Val. Loss: 0.441 |  Val. Acc: 79.41%\n",
      "tensor(0.4380, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.438 | Train Acc: 80.78%\n",
      "\t Val. Loss: 0.438 |  Val. Acc: 79.56%\n",
      "tensor(0.4352, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.435 | Train Acc: 80.90%\n",
      "\t Val. Loss: 0.435 |  Val. Acc: 79.88%\n",
      "tensor(0.4326, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.433 | Train Acc: 81.12%\n",
      "\t Val. Loss: 0.433 |  Val. Acc: 79.88%\n",
      "tensor(0.4300, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.430 | Train Acc: 81.23%\n",
      "\t Val. Loss: 0.430 |  Val. Acc: 80.03%\n",
      "tensor(0.4274, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.427 | Train Acc: 81.41%\n",
      "\t Val. Loss: 0.427 |  Val. Acc: 80.42%\n",
      "tensor(0.4250, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch  40\n",
      "\tTrain Loss: 0.425 | Train Acc: 81.45%\n",
      "\t Val. Loss: 0.425 |  Val. Acc: 80.73%\n",
      "tensor(0.4226, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.423 | Train Acc: 81.56%\n",
      "\t Val. Loss: 0.422 |  Val. Acc: 81.05%\n",
      "tensor(0.4203, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.420 | Train Acc: 81.62%\n",
      "\t Val. Loss: 0.419 |  Val. Acc: 81.12%\n",
      "tensor(0.4181, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.418 | Train Acc: 81.70%\n",
      "\t Val. Loss: 0.417 |  Val. Acc: 81.28%\n",
      "tensor(0.4159, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.416 | Train Acc: 81.74%\n",
      "\t Val. Loss: 0.415 |  Val. Acc: 81.67%\n",
      "tensor(0.4137, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.414 | Train Acc: 81.78%\n",
      "\t Val. Loss: 0.412 |  Val. Acc: 81.98%\n",
      "tensor(0.4117, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.412 | Train Acc: 81.84%\n",
      "\t Val. Loss: 0.410 |  Val. Acc: 81.75%\n",
      "tensor(0.4096, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.410 | Train Acc: 81.90%\n",
      "\t Val. Loss: 0.408 |  Val. Acc: 81.98%\n",
      "tensor(0.4077, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.408 | Train Acc: 81.97%\n",
      "\t Val. Loss: 0.406 |  Val. Acc: 81.98%\n",
      "tensor(0.4057, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.406 | Train Acc: 82.15%\n",
      "\t Val. Loss: 0.404 |  Val. Acc: 82.06%\n",
      "tensor(0.4038, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch  50\n",
      "\tTrain Loss: 0.404 | Train Acc: 82.27%\n",
      "\t Val. Loss: 0.401 |  Val. Acc: 82.14%\n",
      "tensor(0.4020, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.402 | Train Acc: 82.38%\n",
      "\t Val. Loss: 0.399 |  Val. Acc: 82.22%\n",
      "tensor(0.4002, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.400 | Train Acc: 82.52%\n",
      "\t Val. Loss: 0.397 |  Val. Acc: 82.29%\n",
      "tensor(0.3984, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.398 | Train Acc: 82.62%\n",
      "\t Val. Loss: 0.395 |  Val. Acc: 82.29%\n",
      "tensor(0.3967, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.397 | Train Acc: 82.64%\n",
      "\t Val. Loss: 0.394 |  Val. Acc: 82.45%\n",
      "tensor(0.3950, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.395 | Train Acc: 82.75%\n",
      "\t Val. Loss: 0.392 |  Val. Acc: 82.37%\n",
      "tensor(0.3933, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.393 | Train Acc: 82.87%\n",
      "\t Val. Loss: 0.390 |  Val. Acc: 82.76%\n",
      "tensor(0.3917, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.392 | Train Acc: 82.93%\n",
      "\t Val. Loss: 0.388 |  Val. Acc: 82.68%\n",
      "tensor(0.3901, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.390 | Train Acc: 82.89%\n",
      "\t Val. Loss: 0.386 |  Val. Acc: 82.53%\n",
      "tensor(0.3885, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.388 | Train Acc: 82.93%\n",
      "\t Val. Loss: 0.384 |  Val. Acc: 82.53%\n",
      "tensor(0.3869, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch  60\n",
      "\tTrain Loss: 0.387 | Train Acc: 83.01%\n",
      "\t Val. Loss: 0.383 |  Val. Acc: 82.68%\n",
      "tensor(0.3854, grad_fn=<BinaryCrossEntropyBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.385 | Train Acc: 82.97%\n",
      "\t Val. Loss: 0.381 |  Val. Acc: 82.92%\n",
      "tensor(0.3839, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.384 | Train Acc: 83.03%\n",
      "\t Val. Loss: 0.379 |  Val. Acc: 82.84%\n",
      "tensor(0.3824, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.382 | Train Acc: 83.07%\n",
      "\t Val. Loss: 0.378 |  Val. Acc: 83.00%\n",
      "tensor(0.3809, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.381 | Train Acc: 83.20%\n",
      "\t Val. Loss: 0.376 |  Val. Acc: 83.15%\n",
      "tensor(0.3795, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.379 | Train Acc: 83.22%\n",
      "\t Val. Loss: 0.375 |  Val. Acc: 83.23%\n",
      "tensor(0.3781, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.378 | Train Acc: 83.22%\n",
      "\t Val. Loss: 0.373 |  Val. Acc: 83.31%\n",
      "tensor(0.3767, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.377 | Train Acc: 83.24%\n",
      "\t Val. Loss: 0.372 |  Val. Acc: 83.31%\n",
      "tensor(0.3753, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.375 | Train Acc: 83.24%\n",
      "\t Val. Loss: 0.370 |  Val. Acc: 83.31%\n",
      "tensor(0.3739, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.374 | Train Acc: 83.34%\n",
      "\t Val. Loss: 0.369 |  Val. Acc: 83.23%\n",
      "tensor(0.3726, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch  70\n",
      "\tTrain Loss: 0.373 | Train Acc: 83.38%\n",
      "\t Val. Loss: 0.367 |  Val. Acc: 83.23%\n",
      "tensor(0.3713, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.371 | Train Acc: 83.48%\n",
      "\t Val. Loss: 0.366 |  Val. Acc: 83.23%\n",
      "tensor(0.3700, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.370 | Train Acc: 83.52%\n",
      "\t Val. Loss: 0.364 |  Val. Acc: 83.39%\n",
      "tensor(0.3687, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.369 | Train Acc: 83.52%\n",
      "\t Val. Loss: 0.363 |  Val. Acc: 83.46%\n",
      "tensor(0.3675, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.367 | Train Acc: 83.53%\n",
      "\t Val. Loss: 0.362 |  Val. Acc: 83.46%\n",
      "tensor(0.3662, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.366 | Train Acc: 83.55%\n",
      "\t Val. Loss: 0.360 |  Val. Acc: 83.31%\n",
      "tensor(0.3650, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.365 | Train Acc: 83.61%\n",
      "\t Val. Loss: 0.359 |  Val. Acc: 83.46%\n",
      "tensor(0.3638, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.364 | Train Acc: 83.73%\n",
      "\t Val. Loss: 0.358 |  Val. Acc: 83.78%\n",
      "tensor(0.3626, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.363 | Train Acc: 83.77%\n",
      "\t Val. Loss: 0.356 |  Val. Acc: 83.85%\n",
      "tensor(0.3614, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.361 | Train Acc: 83.83%\n",
      "\t Val. Loss: 0.355 |  Val. Acc: 84.01%\n",
      "tensor(0.3602, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch  80\n",
      "\tTrain Loss: 0.360 | Train Acc: 83.85%\n",
      "\t Val. Loss: 0.354 |  Val. Acc: 84.01%\n",
      "tensor(0.3591, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.359 | Train Acc: 83.93%\n",
      "\t Val. Loss: 0.353 |  Val. Acc: 84.17%\n",
      "tensor(0.3579, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.358 | Train Acc: 83.98%\n",
      "\t Val. Loss: 0.352 |  Val. Acc: 84.24%\n",
      "tensor(0.3568, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.357 | Train Acc: 84.02%\n",
      "\t Val. Loss: 0.351 |  Val. Acc: 84.17%\n",
      "tensor(0.3557, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.356 | Train Acc: 84.04%\n",
      "\t Val. Loss: 0.349 |  Val. Acc: 84.40%\n",
      "tensor(0.3546, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.355 | Train Acc: 84.14%\n",
      "\t Val. Loss: 0.348 |  Val. Acc: 84.48%\n",
      "tensor(0.3536, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.354 | Train Acc: 84.28%\n",
      "\t Val. Loss: 0.347 |  Val. Acc: 84.63%\n",
      "tensor(0.3525, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.352 | Train Acc: 84.32%\n",
      "\t Val. Loss: 0.346 |  Val. Acc: 84.63%\n",
      "tensor(0.3514, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.351 | Train Acc: 84.28%\n",
      "\t Val. Loss: 0.345 |  Val. Acc: 84.71%\n",
      "tensor(0.3504, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.350 | Train Acc: 84.33%\n",
      "\t Val. Loss: 0.344 |  Val. Acc: 84.71%\n",
      "tensor(0.3494, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch  90\n",
      "\tTrain Loss: 0.349 | Train Acc: 84.37%\n",
      "\t Val. Loss: 0.343 |  Val. Acc: 84.71%\n",
      "tensor(0.3484, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.348 | Train Acc: 84.37%\n",
      "\t Val. Loss: 0.342 |  Val. Acc: 84.63%\n",
      "tensor(0.3474, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.347 | Train Acc: 84.39%\n",
      "\t Val. Loss: 0.341 |  Val. Acc: 84.71%\n",
      "tensor(0.3464, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.346 | Train Acc: 84.41%\n",
      "\t Val. Loss: 0.340 |  Val. Acc: 84.79%\n",
      "tensor(0.3454, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.345 | Train Acc: 84.41%\n",
      "\t Val. Loss: 0.339 |  Val. Acc: 84.87%\n",
      "tensor(0.3445, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.344 | Train Acc: 84.37%\n",
      "\t Val. Loss: 0.338 |  Val. Acc: 84.87%\n",
      "tensor(0.3435, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.344 | Train Acc: 84.43%\n",
      "\t Val. Loss: 0.337 |  Val. Acc: 85.02%\n",
      "tensor(0.3426, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.343 | Train Acc: 84.43%\n",
      "\t Val. Loss: 0.336 |  Val. Acc: 84.95%\n",
      "tensor(0.3417, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.342 | Train Acc: 84.47%\n",
      "\t Val. Loss: 0.335 |  Val. Acc: 85.02%\n",
      "tensor(0.3408, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.341 | Train Acc: 84.51%\n",
      "\t Val. Loss: 0.335 |  Val. Acc: 85.10%\n",
      "tensor(0.3399, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch  100\n",
      "\tTrain Loss: 0.340 | Train Acc: 84.59%\n",
      "\t Val. Loss: 0.334 |  Val. Acc: 85.18%\n",
      "tensor(0.3390, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.339 | Train Acc: 84.63%\n",
      "\t Val. Loss: 0.333 |  Val. Acc: 85.34%\n",
      "tensor(0.3381, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.338 | Train Acc: 84.71%\n",
      "\t Val. Loss: 0.332 |  Val. Acc: 85.49%\n",
      "tensor(0.3372, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.337 | Train Acc: 84.78%\n",
      "\t Val. Loss: 0.331 |  Val. Acc: 85.49%\n",
      "tensor(0.3364, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.336 | Train Acc: 84.84%\n",
      "\t Val. Loss: 0.330 |  Val. Acc: 85.49%\n",
      "tensor(0.3355, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.336 | Train Acc: 84.86%\n",
      "\t Val. Loss: 0.330 |  Val. Acc: 85.73%\n",
      "tensor(0.3347, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.335 | Train Acc: 84.86%\n",
      "\t Val. Loss: 0.329 |  Val. Acc: 85.73%\n",
      "tensor(0.3339, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.334 | Train Acc: 84.90%\n",
      "\t Val. Loss: 0.328 |  Val. Acc: 85.65%\n",
      "tensor(0.3331, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.333 | Train Acc: 84.94%\n",
      "\t Val. Loss: 0.327 |  Val. Acc: 85.73%\n",
      "tensor(0.3323, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.332 | Train Acc: 84.94%\n",
      "\t Val. Loss: 0.327 |  Val. Acc: 85.88%\n",
      "tensor(0.3315, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch  110\n",
      "\tTrain Loss: 0.331 | Train Acc: 84.96%\n",
      "\t Val. Loss: 0.326 |  Val. Acc: 85.96%\n",
      "tensor(0.3307, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.331 | Train Acc: 84.96%\n",
      "\t Val. Loss: 0.325 |  Val. Acc: 86.04%\n",
      "tensor(0.3299, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.330 | Train Acc: 84.98%\n",
      "\t Val. Loss: 0.324 |  Val. Acc: 86.04%\n",
      "tensor(0.3291, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.329 | Train Acc: 85.00%\n",
      "\t Val. Loss: 0.324 |  Val. Acc: 86.04%\n",
      "tensor(0.3283, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.328 | Train Acc: 85.00%\n",
      "\t Val. Loss: 0.323 |  Val. Acc: 86.04%\n",
      "tensor(0.3276, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.328 | Train Acc: 85.08%\n",
      "\t Val. Loss: 0.322 |  Val. Acc: 86.04%\n",
      "tensor(0.3268, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.327 | Train Acc: 85.21%\n",
      "\t Val. Loss: 0.322 |  Val. Acc: 85.96%\n",
      "tensor(0.3261, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.326 | Train Acc: 85.23%\n",
      "\t Val. Loss: 0.321 |  Val. Acc: 85.96%\n",
      "tensor(0.3254, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.325 | Train Acc: 85.23%\n",
      "\t Val. Loss: 0.320 |  Val. Acc: 86.12%\n",
      "tensor(0.3246, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.325 | Train Acc: 85.27%\n",
      "\t Val. Loss: 0.320 |  Val. Acc: 86.04%\n",
      "tensor(0.3239, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch  120\n",
      "\tTrain Loss: 0.324 | Train Acc: 85.27%\n",
      "\t Val. Loss: 0.319 |  Val. Acc: 86.12%\n",
      "tensor(0.3232, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.323 | Train Acc: 85.23%\n",
      "\t Val. Loss: 0.319 |  Val. Acc: 86.12%\n",
      "tensor(0.3225, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.322 | Train Acc: 85.27%\n",
      "\t Val. Loss: 0.318 |  Val. Acc: 86.27%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3218, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.322 | Train Acc: 85.27%\n",
      "\t Val. Loss: 0.317 |  Val. Acc: 86.19%\n",
      "tensor(0.3211, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.321 | Train Acc: 85.31%\n",
      "\t Val. Loss: 0.317 |  Val. Acc: 86.19%\n",
      "tensor(0.3204, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.320 | Train Acc: 85.33%\n",
      "\t Val. Loss: 0.316 |  Val. Acc: 86.19%\n",
      "tensor(0.3197, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.320 | Train Acc: 85.39%\n",
      "\t Val. Loss: 0.315 |  Val. Acc: 86.27%\n",
      "tensor(0.3190, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.319 | Train Acc: 85.41%\n",
      "\t Val. Loss: 0.315 |  Val. Acc: 86.27%\n",
      "tensor(0.3183, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.318 | Train Acc: 85.45%\n",
      "\t Val. Loss: 0.314 |  Val. Acc: 86.35%\n",
      "tensor(0.3177, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.318 | Train Acc: 85.49%\n",
      "\t Val. Loss: 0.314 |  Val. Acc: 86.35%\n",
      "tensor(0.3170, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch  130\n",
      "\tTrain Loss: 0.317 | Train Acc: 85.52%\n",
      "\t Val. Loss: 0.313 |  Val. Acc: 86.43%\n",
      "tensor(0.3163, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.316 | Train Acc: 85.54%\n",
      "\t Val. Loss: 0.312 |  Val. Acc: 86.51%\n",
      "tensor(0.3156, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.316 | Train Acc: 85.66%\n",
      "\t Val. Loss: 0.312 |  Val. Acc: 86.74%\n",
      "tensor(0.3150, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.315 | Train Acc: 85.66%\n",
      "\t Val. Loss: 0.311 |  Val. Acc: 86.82%\n",
      "tensor(0.3143, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.314 | Train Acc: 85.72%\n",
      "\t Val. Loss: 0.311 |  Val. Acc: 86.90%\n",
      "tensor(0.3137, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.314 | Train Acc: 85.74%\n",
      "\t Val. Loss: 0.310 |  Val. Acc: 86.97%\n",
      "tensor(0.3130, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.313 | Train Acc: 85.80%\n",
      "\t Val. Loss: 0.310 |  Val. Acc: 86.97%\n",
      "tensor(0.3123, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.312 | Train Acc: 85.91%\n",
      "\t Val. Loss: 0.309 |  Val. Acc: 86.97%\n",
      "tensor(0.3117, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.312 | Train Acc: 85.99%\n",
      "\t Val. Loss: 0.308 |  Val. Acc: 87.05%\n",
      "tensor(0.3110, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.311 | Train Acc: 86.07%\n",
      "\t Val. Loss: 0.308 |  Val. Acc: 87.05%\n",
      "tensor(0.3104, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch  140\n",
      "\tTrain Loss: 0.310 | Train Acc: 86.09%\n",
      "\t Val. Loss: 0.307 |  Val. Acc: 87.13%\n",
      "tensor(0.3097, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.310 | Train Acc: 86.15%\n",
      "\t Val. Loss: 0.307 |  Val. Acc: 87.21%\n",
      "tensor(0.3091, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.309 | Train Acc: 86.17%\n",
      "\t Val. Loss: 0.306 |  Val. Acc: 87.21%\n",
      "tensor(0.3084, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.308 | Train Acc: 86.25%\n",
      "\t Val. Loss: 0.306 |  Val. Acc: 87.29%\n",
      "tensor(0.3078, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.308 | Train Acc: 86.32%\n",
      "\t Val. Loss: 0.305 |  Val. Acc: 87.36%\n",
      "tensor(0.3071, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.307 | Train Acc: 86.32%\n",
      "\t Val. Loss: 0.304 |  Val. Acc: 87.36%\n",
      "tensor(0.3065, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.306 | Train Acc: 86.32%\n",
      "\t Val. Loss: 0.304 |  Val. Acc: 87.29%\n",
      "tensor(0.3058, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.306 | Train Acc: 86.40%\n",
      "\t Val. Loss: 0.303 |  Val. Acc: 87.44%\n",
      "tensor(0.3051, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.305 | Train Acc: 86.38%\n",
      "\t Val. Loss: 0.303 |  Val. Acc: 87.36%\n",
      "tensor(0.3045, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.304 | Train Acc: 86.44%\n",
      "\t Val. Loss: 0.302 |  Val. Acc: 87.36%\n",
      "tensor(0.3038, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch  150\n",
      "\tTrain Loss: 0.304 | Train Acc: 86.54%\n",
      "\t Val. Loss: 0.302 |  Val. Acc: 87.52%\n",
      "tensor(0.3032, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.303 | Train Acc: 86.56%\n",
      "\t Val. Loss: 0.301 |  Val. Acc: 87.68%\n",
      "tensor(0.3025, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.303 | Train Acc: 86.58%\n",
      "\t Val. Loss: 0.300 |  Val. Acc: 87.68%\n",
      "tensor(0.3019, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.302 | Train Acc: 86.66%\n",
      "\t Val. Loss: 0.300 |  Val. Acc: 87.68%\n",
      "tensor(0.3012, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.301 | Train Acc: 86.71%\n",
      "\t Val. Loss: 0.299 |  Val. Acc: 87.75%\n",
      "tensor(0.3005, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.301 | Train Acc: 86.75%\n",
      "\t Val. Loss: 0.299 |  Val. Acc: 87.99%\n",
      "tensor(0.2999, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.300 | Train Acc: 86.77%\n",
      "\t Val. Loss: 0.298 |  Val. Acc: 87.99%\n",
      "tensor(0.2992, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.299 | Train Acc: 86.81%\n",
      "\t Val. Loss: 0.297 |  Val. Acc: 88.07%\n",
      "tensor(0.2985, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.299 | Train Acc: 86.87%\n",
      "\t Val. Loss: 0.297 |  Val. Acc: 87.99%\n",
      "tensor(0.2979, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.298 | Train Acc: 86.95%\n",
      "\t Val. Loss: 0.296 |  Val. Acc: 87.99%\n",
      "tensor(0.2972, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch  160\n",
      "\tTrain Loss: 0.297 | Train Acc: 86.97%\n",
      "\t Val. Loss: 0.296 |  Val. Acc: 88.07%\n",
      "tensor(0.2965, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.297 | Train Acc: 86.95%\n",
      "\t Val. Loss: 0.295 |  Val. Acc: 88.22%\n",
      "tensor(0.2959, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.296 | Train Acc: 86.99%\n",
      "\t Val. Loss: 0.295 |  Val. Acc: 88.38%\n",
      "tensor(0.2952, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.295 | Train Acc: 87.01%\n",
      "\t Val. Loss: 0.294 |  Val. Acc: 88.38%\n",
      "tensor(0.2945, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.294 | Train Acc: 87.07%\n",
      "\t Val. Loss: 0.293 |  Val. Acc: 88.38%\n",
      "tensor(0.2938, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.294 | Train Acc: 87.07%\n",
      "\t Val. Loss: 0.293 |  Val. Acc: 88.38%\n",
      "tensor(0.2931, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.293 | Train Acc: 87.12%\n",
      "\t Val. Loss: 0.292 |  Val. Acc: 88.38%\n",
      "tensor(0.2924, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.292 | Train Acc: 87.18%\n",
      "\t Val. Loss: 0.291 |  Val. Acc: 88.46%\n",
      "tensor(0.2918, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.292 | Train Acc: 87.18%\n",
      "\t Val. Loss: 0.291 |  Val. Acc: 88.46%\n",
      "tensor(0.2911, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.291 | Train Acc: 87.24%\n",
      "\t Val. Loss: 0.290 |  Val. Acc: 88.46%\n",
      "tensor(0.2904, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch  170\n",
      "\tTrain Loss: 0.290 | Train Acc: 87.24%\n",
      "\t Val. Loss: 0.290 |  Val. Acc: 88.38%\n",
      "tensor(0.2897, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.290 | Train Acc: 87.34%\n",
      "\t Val. Loss: 0.289 |  Val. Acc: 88.53%\n",
      "tensor(0.2890, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.289 | Train Acc: 87.32%\n",
      "\t Val. Loss: 0.288 |  Val. Acc: 88.53%\n",
      "tensor(0.2883, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.288 | Train Acc: 87.40%\n",
      "\t Val. Loss: 0.288 |  Val. Acc: 88.53%\n",
      "tensor(0.2876, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.288 | Train Acc: 87.46%\n",
      "\t Val. Loss: 0.287 |  Val. Acc: 88.61%\n",
      "tensor(0.2869, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.287 | Train Acc: 87.53%\n",
      "\t Val. Loss: 0.287 |  Val. Acc: 88.77%\n",
      "tensor(0.2862, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.286 | Train Acc: 87.51%\n",
      "\t Val. Loss: 0.286 |  Val. Acc: 88.69%\n",
      "tensor(0.2856, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.286 | Train Acc: 87.61%\n",
      "\t Val. Loss: 0.285 |  Val. Acc: 88.77%\n",
      "tensor(0.2849, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.285 | Train Acc: 87.63%\n",
      "\t Val. Loss: 0.285 |  Val. Acc: 88.77%\n",
      "tensor(0.2842, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.284 | Train Acc: 87.69%\n",
      "\t Val. Loss: 0.284 |  Val. Acc: 88.69%\n",
      "tensor(0.2835, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch  180\n",
      "\tTrain Loss: 0.283 | Train Acc: 87.73%\n",
      "\t Val. Loss: 0.284 |  Val. Acc: 88.69%\n",
      "tensor(0.2828, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.283 | Train Acc: 87.79%\n",
      "\t Val. Loss: 0.283 |  Val. Acc: 88.69%\n",
      "tensor(0.2821, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.282 | Train Acc: 87.75%\n",
      "\t Val. Loss: 0.282 |  Val. Acc: 88.85%\n",
      "tensor(0.2814, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.281 | Train Acc: 87.75%\n",
      "\t Val. Loss: 0.282 |  Val. Acc: 88.85%\n",
      "tensor(0.2808, grad_fn=<BinaryCrossEntropyBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.281 | Train Acc: 87.87%\n",
      "\t Val. Loss: 0.281 |  Val. Acc: 88.92%\n",
      "tensor(0.2801, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.280 | Train Acc: 87.94%\n",
      "\t Val. Loss: 0.281 |  Val. Acc: 88.92%\n",
      "tensor(0.2794, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.279 | Train Acc: 88.00%\n",
      "\t Val. Loss: 0.280 |  Val. Acc: 89.00%\n",
      "tensor(0.2788, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.279 | Train Acc: 88.06%\n",
      "\t Val. Loss: 0.279 |  Val. Acc: 89.08%\n",
      "tensor(0.2781, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.278 | Train Acc: 88.10%\n",
      "\t Val. Loss: 0.279 |  Val. Acc: 89.08%\n",
      "tensor(0.2774, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.277 | Train Acc: 88.12%\n",
      "\t Val. Loss: 0.278 |  Val. Acc: 89.24%\n",
      "tensor(0.2768, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch  190\n",
      "\tTrain Loss: 0.277 | Train Acc: 88.16%\n",
      "\t Val. Loss: 0.278 |  Val. Acc: 89.24%\n",
      "tensor(0.2761, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.276 | Train Acc: 88.20%\n",
      "\t Val. Loss: 0.277 |  Val. Acc: 89.31%\n",
      "tensor(0.2755, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.275 | Train Acc: 88.18%\n",
      "\t Val. Loss: 0.277 |  Val. Acc: 89.39%\n",
      "tensor(0.2749, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.275 | Train Acc: 88.16%\n",
      "\t Val. Loss: 0.276 |  Val. Acc: 89.39%\n",
      "tensor(0.2742, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.274 | Train Acc: 88.14%\n",
      "\t Val. Loss: 0.276 |  Val. Acc: 89.47%\n",
      "tensor(0.2736, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.274 | Train Acc: 88.24%\n",
      "\t Val. Loss: 0.275 |  Val. Acc: 89.39%\n",
      "tensor(0.2730, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.273 | Train Acc: 88.28%\n",
      "\t Val. Loss: 0.275 |  Val. Acc: 89.39%\n",
      "tensor(0.2724, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.272 | Train Acc: 88.29%\n",
      "\t Val. Loss: 0.274 |  Val. Acc: 89.47%\n",
      "tensor(0.2718, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.272 | Train Acc: 88.35%\n",
      "\t Val. Loss: 0.274 |  Val. Acc: 89.47%\n",
      "tensor(0.2712, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "\tTrain Loss: 0.271 | Train Acc: 88.37%\n",
      "\t Val. Loss: 0.273 |  Val. Acc: 89.47%\n"
     ]
    }
   ],
   "source": [
    "net = Net(300, lstm_units = max_len, batch_size = len(X_train_t), lstm_layers = 1, bidirectional = True, hidden_dim=128, dropout = 0.5)\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.1)\n",
    "#optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "epochs = 200\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    predictions = net.forward(X_train_t, max_len)\n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion(predictions.reshape(-1), y_train_t)\n",
    "    acc = accuracy(predictions.reshape(-1), y_train_t) \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(loss)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        predictions_valid = net(X_test_t, max_len).reshape(-1)\n",
    "        loss_valid = criterion(predictions_valid, y_test_t)\n",
    "        acc_valid = accuracy(predictions_valid, y_test_t)\n",
    "    \n",
    "    if loss_valid < best_valid_loss:\n",
    "        best_valid_loss = loss_valid\n",
    "        torch.save(net.state_dict(), 'saved_weights'+'.pt')\n",
    "    if epoch%10 == 0: print('Epoch ', epoch)\n",
    "    print(f'\\tTrain Loss: {loss:.3f} | Train Acc: {acc * 100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {loss_valid:.3f} |  Val. Acc: {acc_valid * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вывод: на 200х эпохах сеть получаила точно 87,4%. судя по близким значениям train и validate можно еще тренировать и получить более высокие результаты. Важно отметить, что при использовании одностороннего lstm модель уперлась в потолок точности 72%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([[1, 1],\n",
       "        [7, 7],\n",
       "        [5, 5],\n",
       "        [2, 2],\n",
       "        [8, 8],\n",
       "        [6, 6],\n",
       "        [3, 3],\n",
       "        [9, 9],\n",
       "        [4, 4]]), batch_sizes=tensor([3, 3, 2, 1]), sorted_indices=tensor([2, 0, 1]), unsorted_indices=tensor([1, 2, 0]))"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#пример использования pad_sequence и пр\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "samp = [torch.tensor([[7,7],[8,8],[9,9]]), torch.tensor([[5,5], [6,6]]), torch.tensor([[1,1],[2,2],[3,3], [4,4]])]\n",
    "\n",
    "#packed_input = pack_padded_sequence(text_emb, text_len, batch_first=True, enforce_sorted=False)\n",
    "ps = pad_sequence(samp, batch_first=True)\n",
    "text_len = ps.size()\n",
    "\n",
    "pps = pack_padded_sequence(ps, [3,2,4], batch_first=True, enforce_sorted=False)\n",
    "pps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 2])"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 1],\n",
       "         [2, 2],\n",
       "         [3, 3]],\n",
       "\n",
       "        [[5, 5],\n",
       "         [6, 6],\n",
       "         [0, 0]]])"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[1, 1],\n",
       "          [2, 2],\n",
       "          [3, 3]],\n",
       " \n",
       "         [[5, 5],\n",
       "          [6, 6],\n",
       "          [0, 0]]]), tensor([3, 2]))"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output, _ = pad_packed_sequence(pps, batch_first=True, total_length=3)\n",
    "pad_packed_sequence(pps, batch_first=True, total_length=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 1],\n",
       "         [2, 2],\n",
       "         [3, 3]],\n",
       "\n",
       "        [[5, 5],\n",
       "         [6, 6],\n",
       "         [0, 0]]])"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
